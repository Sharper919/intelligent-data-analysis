{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Лекція 5. Композиції (ансамблі) алгоритмів</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зміст \n",
    "\n",
    "- [5.1. Ансамблі алгоритмів](#5.1)\n",
    "    + [5.1.1. Формалізація ансамблів](#5.1.1)\n",
    "    + [5.1.2. Бутстреп](#5.1.2)\n",
    "    + [5.1.3. Беггінг](#5.1.3)\n",
    "    + [5.1.4. Out-of-bag error](#5.1.4)   \n",
    "- [5.2. Випадковий ліс](#5.2)\n",
    "    + [5.2.1. Алгоритм випадкового лісу](#5.2.1)\n",
    "    + [5.2.2. Порівняння з деревом рішень і беггінгом](#5.2.2)\n",
    "    + [5.2.3. Параметри випадкового лісу](#5.2.3)\n",
    "    + [5.2.4. Варіація й декорреляціонний ефект](#5.2.4)\n",
    "    + [5.2.5. Зміщення](#5.2.5)\n",
    "    + [5.2.6. Понад випадкові дерева](#5.2.6)\n",
    "    + [5.2.7. Схожість випадкового лісу з алгоритмом *k*-найближчих сусідів](#5.2.7)\n",
    "    + [5.2.8. Перетворення ознак в багатовимірний простір](#5.2.8)    \n",
    "- [5.3. Важливість ознак на прикладі випадкового лісу](#5.3)\n",
    "    + [5.3.1. Метод випадкового лісу](#5.3.1)\n",
    "    + [5.3.2. Приклад розрахування важливості ознак](#5.3.2)\n",
    "- [5.4. Переваги та недоліки використання випадкових лісів в ІАД та МН](#5.4)\n",
    "- [5.5. Корисні посилання](#5.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "З попередніх лекцій ви вже дізналися про різні алгоритми класифікації, а також навчилися правильно валідувати й оцінювати якість моделі. Проте, що робити, якщо ви вже отримали хорошу модель, але далі не вдається покращити кількісні метрики? У такому випадку потрібно застосувати більш просунуті техніки машинного навчання (МН), які можна об'єднати словом «ансамблі».\n",
    "\n",
    "Ансамбль – це певна сукупність, частини якої утворюють єдине ціле. Зі звичного життя ви, можливо, чули за музичні ансамблі, де об'єднуються кілька музичних інструментів, архітектурні ансамблі з різними будівлями тощо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для виконання подальших прикладів спершу підключимо основні бібліотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# Відключимо попередження Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Задамо графіки у форматі .svg, щоби вони мали кращу чіткість\n",
    "# %config InlineBackend.figure_format = 'svg' \n",
    "\n",
    "# Підвищимо розмір графіків за замовчуванням\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "\n",
    "#from drawdata import draw_scatter, draw_line, draw_histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.1. Ансамблі алгоритмів</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.1.1. Формалізація ансамблів</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошим прикладом ансамблів вважається теорема Кондорсе «про суд присяжних» (1784) ([парадокс Кондорсе](https://uk.wikipedia.org/wiki/%D0%9F%D0%B0%D1%80%D0%B0%D0%B4%D0%BE%D0%BA%D1%81_%D0%9A%D0%BE%D0%BD%D0%B4%D0%BE%D1%80%D1%81%D0%B5)). Якщо кожен член суду присяжних має незалежну думку, і якщо ймовірність правильного рішення члена суду присяжних більше 0.5, то тоді ймовірність правильного рішення присяжних загалом зростає зі збільшенням кількості членів суду і прямує до одиниці. Якщо ж ймовірність бути правим у кожного з членів журі менше 0.5, то ймовірність прийняття правильного рішення присяжними в цілому монотонно зменшується і прямує до нуля зі збільшенням кількості присяжних."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формалізуємо цю задачу. Нехай:\n",
    "\n",
    "- $N$ – кількість присяжних;\n",
    "- $p$ – ймовірність правильного рішення присяжного;\n",
    "- $m$ – мінімальна більшість членів суду присяжних, $ m = floor(\\frac{N}{2}) + 1 $;\n",
    "- $C_N^i$ – кількість [комбінацій](https://uk.wikipedia.org/wiki/%D0%9A%D0%BE%D0%BC%D0%B1%D1%96%D0%BD%D0%B0%D1%86%D1%96%D1%8F_(%D0%BA%D0%BE%D0%BC%D0%B1%D1%96%D0%BD%D0%B0%D1%82%D0%BE%D1%80%D0%B8%D0%BA%D0%B0)) з $N$ по $i$.\n",
    "\n",
    "Тоді ймовірність правильного рішення всього суду присяжних:\n",
    "\n",
    "$$\\large \\mu = \\sum_{i=m}^{N}C_N^ip^i(1-p)^{N-i}, $$\n",
    "\n",
    "якщо $p > 0.5$, то $\\mu > p; $ <br>\n",
    "якщо $N \\rightarrow \\infty $, то $\\mu \\rightarrow 1; $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо ще один приклад ансамблів – \"Мудрість натовпу\", [PDF](https://www.mann-ivanov-ferber.ru/assets/files/bookparts/the_wisdom_of_crowds/mudrost_read.pdf). Френсіс Гальтон в 1906 році відвідав ринок, де проводилася лотерея для мешканців села. Зібралося близько 800 селян, і вони намагалися вгадати вагу бика, що стояв перед ними. Його вага становила 1198 фунтів. Жоден селянин не вгадав точну вагу бика, але якщо порахувати середнє від їхніх прогнозів, то отримаємо 1197 фунтів. Надалі цю ідею зменшення помилки застосували до методів МН."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.1.2. Бутстреп</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging (від Bootstrap aggregation) – це один з перших і найпростіших видів ансамблів. Він був придуманий [Ле́о Бре́йманом](https://en.wikipedia.org/wiki/Leo_Breiman) в 1994 році. Беггінг ґрунтується на статистичному методі бутстреппінга, який дає змогу оцінювати багато статистик складних моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод бутстрепа полягає в наступному.\n",
    "\n",
    "Нехай є вибірка $X$ розміру $N$. Рівномірно візьмемо з вибірки $N$ об'єктів з поверненням. Це означає, що ми будемо $N$ разів вибирати довільний об'єкт вибірки (вважаємо, що кожен об'єкт «дістається» з однаковою ймовірністю $\\frac{1}{N}$), водночас кожен раз ми вибираємо з усіх початкових $N$ об'єктів. Можемо уявити мішок, з якого дістають кульки: обрана на певному етапі кулька повертається назад в мішок, і наступний вибір знову робиться з однаковою ймовірністю з того ж числа кульок. Зазначимо, що через повернення кульок у мішок ми можемо отримувати повтори. Позначимо нову вибірку через $X_1$. Повторюючи процедуру $M$ разів, сформуємо $M$ підвибірок $X_1, \\dots, X_M$. Тепер ми маємо досить велику кількість вибірок і можемо оцінювати різні статистики початкового розподілу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_1_bootstrap_eng.png\" align=\"center\" width=100% height=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для прикладу візьмемо вже добре нам відомий набір даних `telecom_churn`. Нагадаємо, що це задача бінарної класифікації відтоку клієнтів. Одним з найважливіших ознак в цьому наборі даних є кількість дзвінків в сервісний центр, які були зроблені клієнтом. Спробуємо візуалізувати дані й поглянемо на розподіл даної ознаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telecom_churn_url = 'https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/01_lecture-notes/ida_lecture-05_compositions/telecom_churn.csv'\n",
    "\n",
    "telecom_data = pd.read_csv(telecom_churn_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.kdeplot(telecom_data[telecom_data['Churn'] == False]['Customer service calls'], label = 'Loyal', color='blue')\n",
    "fig = sns.kdeplot(telecom_data[telecom_data['Churn'] == True]['Customer service calls'], label = 'Churn', color='red')        \n",
    "fig.set(xlabel='Кількість дзвінків', ylabel='Щільність')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як ви вже могли помітити, кількість дзвінків в сервісний центр у лояльних клієнтів менше, ніж у колишніх клієнтів. Тепер було б добре оцінити скільки в середньому робить дзвінків кожна з груп. Оскільки даних в нашому наборі мало, то шукати середнє не є раціональним рішенням. Натомість краще застосувати метод бутстрепу, про який ми щойно дізналися. Давайте сформуємо 1000 нових підвибірок з генеральної сукупності й зробимо інтервальну оцінку середнього."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples):\n",
    "    # Функція для формування підвибірок за допомогою бутстрепу\n",
    "    my_indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[my_indices]\n",
    "    return samples\n",
    "\n",
    "def stat_intervals(stat, alpha):\n",
    "    # функція для інтервальної оцінки\n",
    "    boundaries = np.percentile(stat, [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зберігаємо в окремі numpy масиви дані за лояльними та вже колишніми клієнтами\n",
    "loyal_calls = telecom_data[telecom_data['Churn'] == False]['Customer service calls'].values\n",
    "churn_calls= telecom_data[telecom_data['Churn'] == True]['Customer service calls'].values\n",
    "\n",
    "# Ставимо seed для відтворюваності результатів\n",
    "np.random.seed(0)\n",
    "\n",
    "# Формуємо вибірки за допомогою бутстрепу й відразу обраховуємо за кожною з них середнє\n",
    "loyal_mean_scores = [np.mean(sample) \n",
    "                       for sample in get_bootstrap_samples(loyal_calls, 1000)]\n",
    "churn_mean_scores = [np.mean(sample) \n",
    "                       for sample in get_bootstrap_samples(churn_calls, 1000)]\n",
    "\n",
    "# Виводимо інтервальну оцінку середнього\n",
    "print(f\"Сервісні дзвінки від лояльних клієнтів (Loyal): середній інтервал {stat_intervals(loyal_mean_scores, 0.05)}\")\n",
    "print(f\"Сервісні дзвінки від колишніх клієнтів (Churn): середній інтервал {stat_intervals(churn_mean_scores, 0.05)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У підсумку ми отримали, що з 95% ймовірністю середня кількість дзвінків від лояльних клієнтів перебуватиме в проміжку між 1.40 і 1.50; водночас колишні клієнти телефонували в середньому від 2.06 до 2.40 разів. Також варто звернути увагу на те, що інтервал для лояльних клієнтів має довжину лише 0.1, що досить логічно, оскільки вони дзвонять рідко (переважно 0, 1 або 2 рази), а незадоволені клієнти будуть дзвонити значно частіше, але з часом їхнє терпіння закінчиться, і вони поміняють оператора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.1.3. Беггінг</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маючи загальне уявлення про бустреп, тепер перейдемо безпосередньо до розгляду методу беггінгу.\n",
    "\n",
    "Нехай є навчальна вибірка $X$. За допомогою бутстрепу сформуємо з неї підвибірки $X_1, \\dots, X_M$. Тепер за кожною підвибіркою навчимо свій класифікатор $a_i(x)$. Підсумковий класифікатор буде усереднювати відповіді всіх цих алгоритмів (у випадку класифікації такий підхід відповідає голосуванню):\n",
    "\n",
    "$$\\large a(x) = \\frac{1}{M}\\sum_{i = 1}^M a_i(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схему беггінгу подамо рисунком:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_2_bagging.png\" align=\"center\" width=100% height=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо задачу регресії з базовими алгоритмами $b_1(x), \\dots , b_n(x)$. Припустимо, що існує справжня функція відповіді для всіх об'єктів $y(x)$, а також задано розподіл на об'єктах $p(x)$. Тоді ми можемо записати помилку кожної функції регресії, як\n",
    "\n",
    "$$ \\large \\varepsilon_i(x) = b_i(x) − y(x),  i = 1, \\dots, n$$\n",
    "\n",
    "і подати математичне очікування середньоквадратичної помилки\n",
    "\n",
    "$$ \\large E_x(b_i(x) − y(x))^{2} = E_x \\varepsilon_i (x).$$\n",
    "\n",
    "Середня помилка побудованих функцій регресії має вигляд\n",
    "\n",
    "$$ \\large E_1 = \\frac{1}{n}E_x \\sum_{i=1}^n \\varepsilon_i^{2}(x). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Припустимо, що помилки незміщені й некорельовані: \n",
    "\n",
    "$$ \\large \\begin{array}{rcl} E_x\\varepsilon_i(x) &=& 0, \\\\\n",
    "E_x\\varepsilon_i(x)\\varepsilon_j(x) &=& 0, i \\neq j. \\end{array}$$\n",
    "\n",
    "Побудуємо тепер нову функцію регресії, яка усереднюватиме відповіді побудованих нами функцій:\n",
    "\n",
    "$$ \\large a(x) = \\frac{1}{n}\\sum_{i=1}^{n}b_i(x). $$\n",
    "\n",
    "Знайдемо її середньоквадратичну помилку:\n",
    "\n",
    "$$ \\large \\begin{array}{rcl}E_n &=& E_x\\Big(\\frac{1}{n}\\sum_{i=1}^{n}b_i(x)-y(x)\\Big)^2 \\\\\n",
    "&=& E_x\\Big(\\frac{1}{n}\\sum_{i=1}^{n}\\varepsilon_i\\Big)^2 \\\\\n",
    "&=& \\frac{1}{n^2}E_x\\Big(\\sum_{i=1}^{n}\\varepsilon_i^2(x) + \\sum_{i \\neq j}\\varepsilon_i(x)\\varepsilon_j(x)\\Big) \\\\\n",
    "&=& \\frac{1}{n}E_1\\end{array}. $$\n",
    "\n",
    "Отже, усереднення відповідей дало змогу зменшити середній квадрат помилки в $n$ разів!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загальна помилка розкладається так:\n",
    "\n",
    "$$ \\large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\vec{x}\\right) &=& \\mathbb{E}\\left[\\left(y – \\hat{f}\\left(\\vec{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\sigma^2 + f^2 + \\text{Var}\\left(\\hat{f}\\right) + \\mathbb{E}\\left[\\hat{f}\\right]^2 – 2f\\mathbb{E}\\left[\\hat{f}\\right] \\\\\n",
    "&=& \\left(f – \\mathbb{E}\\left[\\hat{f}\\right]\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2 \\\\\n",
    "&=& \\text{Bias}\\left(\\hat{f}\\right)^2 + \\text{Var}\\left(\\hat{f}\\right) + \\sigma^2\n",
    "\\end{array}. $$\n",
    "\n",
    "Беггінг дає змогу знизити дисперсію (variance) класифікатора в процесі навчання, зменшуючи величину, на яку помилка буде відрізнятися, якщо навчати модель за різними наборами даними. Іншими словами, беггінг запобігає перенавчанню. Ефективність беггінга досягається завдяки тому, що базові алгоритми, які пройшли навчання за різними підвибірками, виходять досить різними, і їхні помилки взаємно компенсуються під час голосування, а також за рахунок того, що об'єкти-викиди можуть не потрапляти до деяких навчальних підвибірок.\n",
    "\n",
    "У бібліотеці `scikit-learn` реалізовано `BaggingRegressor` і `BaggingClassifier`, які дають нагоду використовувати різні алгоритми МН \"з коробки\".\n",
    "\n",
    "Розглянемо на практиці як працює беггінг і порівняємо його з деревом рішень, користуючись прикладом з [документації](http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_3_tree-vs-bagging_eng.png\" align=\"center\" width=90% height=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помилка дерева рішень:\n",
    "\n",
    "$$ \\large 0.0255 (Err) = 0.0003 (Bias^2)  + 0.0152 (Var) + 0.0098 (\\sigma^2). $$\n",
    "\n",
    "Помилка беггінга:\n",
    "$$ \\large 0.0196 (Err) = 0.0004 (Bias^2)  + 0.0092 (Var) + 0.0098 (\\sigma^2). $$\n",
    "\n",
    "За графіком і результатами вище видно, що помилка дисперсії набагато менше для беггінгу, як ми і довели теоретично вище.\n",
    "\n",
    "Беггінг є ефективним на малих вибірках, коли виключення навіть малої частини навчальних об'єктів призводить до побудови істотно різних базових класифікаторів. У разі великих вибірок зазвичай формують підвибірки істотно меншої довжини.\n",
    "\n",
    "Варто зазначити, що розглянутий нами приклад не надто застосовуваний на практиці, оскільки ми зробили припущення щодо некорельованості помилок, що вкрй рідко трапляється з реальними наборами даних. Якщо це припущення не правильне, то зменшення помилки виявляється не таким значним. Далі ми розглянемо більш складні методи об'єднання алгоритмів в композицію, які дають змогу досягти високих значень метрик в реальних задачах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.1.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.1.4. Out-of-bag error</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Забігаючи наперед, відзначимо, що під час використання випадкових лісів нема потреби використовувати крос-валідацію або окремий тестовий набір, щоб отримати незміщенну оцінку помилки набору тестів. Внутрішню оцінку під час роботи отримують у такий спосіб:\n",
    "\n",
    "Кожне дерево будується з використанням різних зразків бутстрепа з початкових даних. Приблизно 37% прикладів залишаються поза вибіркою бутстрепа і не використовується для побудови $k$-го дерева.\n",
    "\n",
    "Наведене вище твердження легко доводиться. Нехай у вибірці є $ell$ об'єктів. На кожному кроці всі об'єкти потрапляють в підвибірку з поверненням з однаковою ймовірністю, тобто окремий об'єкт – з ймовірністю $\\frac{1}{\\ell}.$ Імовірність того, що об'єкт НЕ потрапить в підвибірку (тобто його не взяли $\\ell$ разів):\n",
    "\n",
    "$$ \\large (1 – \\frac{1}{\\ell})^\\ell. $$\n",
    "\n",
    "За $\\ell \\rightarrow +\\infty$ отримуємо одну з \"чудових\" меж $\\frac{1}{e}$. Тоді ймовірність попадання конкретного об'єкта в підвибірку $\\approx  1 – \\frac{1}{e} \\approx 63\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер подамо вище описане твердження схематично:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_4_oob.png\" align=\"center\" width=100% height=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На рисунку бачимо, що наш класифікатор помилився в 4 спостереженнях, які ми не використали для навчання. Це означає, що точність нашого класифікатора становить:\n",
    "\n",
    "$$\\large \\frac{11}{15}*100\\% = 73.33\\%.$$\n",
    "\n",
    "Виходить, що кожен базовий алгоритм навчається за ~63% початковими об'єктами. Відповідно, на ~37% об'єктах, що залишилися, алгоритм можна відразу перевіряти. Out-of-Bag оцінка – це усереднена оцінка базових алгоритмів на тих ~37% даних, за якими вони не навчалися."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.2. Випадковий ліс</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лео Брейман знайшов застосування бутстрепу не тільки в статистиці, а й в інтелектуальному аналізі даних (ІАД). Він разом з Адель Катлер удосконалив алгоритм випадкового лісу, який був запропонований дослідником в лабораторії IBM [Тін Кам Хо](https://en.wikipedia.org/wiki/Tin_Kam_Ho), додавши до початкового варіанту побудову некорельованих дерев на основі алгоритму [CART](https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff), в поєднанні з методом випадкових підпросторів і беггінгу.\n",
    "\n",
    "Дерева рішень є хорошим сімейством базових класифікаторів для беггінгу, оскільки вони досить складні і водночас можуть досягати нульової помилки за будь-якої вибірки. Метод випадкових підпросторів дає змогу знизити корельованість між деревами й уникнути перенавчання. Базові алгоритми навчаються за різними підмножинами ознакового опису, які також виділяються випадковим чином."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.1. Алгоритм випадкового лісу</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамбль моделей, що використовують метод випадкового підпростору, можна побудувати, використовуючи такий алгоритм:\n",
    "\n",
    "1. Нехай кількість об'єктів для навчання дорівнює $N$, а кількість ознак – $D$.\n",
    "2. Оберіть $L$ за кількість окремих моделей в ансамблі.\n",
    "3. Для кожної окремої моделі $l$ виберіть ознаки кількістю $dl (dl <D)$ як кількість ознак для $l$. Зазвичай для всіх моделей використовується тільки одне значення $dl$.\n",
    "4. Для кожної окремої моделі $l$ створіть навчальну вибірку, вибравши $dl$ ознак поміж $D$ із заміною та виконайте навчання моделі."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер, щоб застосувати модель ансамблю до нового об'єкту, об'єднайте результати окремих $L$ моделей за допомогою мажоритарного голосування або через комбінування [апостеріорних ймовірностей](https://uk.wikipedia.org/wiki/%D0%90%D0%BF%D0%BE%D1%81%D1%82%D0%B5%D1%80%D1%96%D0%BE%D1%80%D0%BD%D0%B0_%D0%B9%D0%BC%D0%BE%D0%B2%D1%96%D1%80%D0%BD%D1%96%D1%81%D1%82%D1%8C#:~:text=%D0%92%20%D0%B1%D0%B0%D1%94%D1%81%D0%BE%D0%B2%D1%96%D0%B9%20%D1%81%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D1%86%D1%96%20%D0%B0%D0%BF%D0%BE%D1%81%D1%82%D0%B5%D1%80%D1%96%D0%BE%CC%81%D1%80%D0%BD%D0%B0%20%D0%B9%D0%BC%D0%BE%D0%B2%D1%96%CC%81%D1%80%D0%BD%D1%96%D1%81%D1%82%D1%8C%20(%D0%B0%D0%BD%D0%B3%D0%BB.&text=posterior%20probability%20distribution)%20%E2%80%94%20%D1%86%D0%B5%20%D1%80%D0%BE%D0%B7%D0%BF%D0%BE%D0%B4%D1%96%D0%BB,%D0%BE%D1%82%D1%80%D0%B8%D0%BC%D0%B0%D0%BD%D0%B8%D0%BC%20%D0%B7%20%D0%B5%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D1%83%20%D0%B0%D0%B1%D0%BE%20%D1%81%D0%BF%D0%BE%D1%81%D1%82%D0%B5%D1%80%D0%B5%D0%B6%D0%B5%D0%BD%D0%BD%D1%8F.).\n",
    "\n",
    "Алгоритм побудови випадкового лісу, що складається з $N$ дерев, є таким:\n",
    "* Для кожного $n = 1, \\dots, N$:\n",
    "     * Сформувати вибірку $X_n$ за допомогою bootstrap.\n",
    "     * Побудувати дерево рішень $b_n$ за вибіркою $X_n$:\n",
    "         - за заданим критерієм обирається краща ознака, виконується розбиття в дереві за цією ознакою до закінчення вибірки;\n",
    "         - дерево будується поки в кожному листі не більше $n_\\text{min}$ об'єктів або поки не досягається наперед задана глибина дерева;\n",
    "         - за кожного розбиття спочатку вибирається $m$ випадкових ознак з $n$ початкових, і далі лише поміж них шукається оптимальний розподіл вибірки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вирішальний класифікатор:\n",
    "\n",
    "$$ \\large a(x) = \\frac{1}{N}\\sum_{i = 1}^N b_i(x). $$\n",
    "\n",
    "Іншими словами, для задачі класифікації ми вибираємо рішення голосуванням за більшістю, а в задачі регресії – за середнім.\n",
    "\n",
    "Рекомендується в задачах класифікації брати $m = \\sqrt{n}$, а в задачах регресії – $m = \\frac{n}{3}$, де $n$ – кількість ознак. Також рекомендується в задачах класифікації будувати кожне дерево до тих пір, поки в кожному листі не виявиться по одному об'єкту, а в задачах регресії – поки в кожному листі не виявиться по п'ять об'єктів.\n",
    "\n",
    "Отже, випадковий ліс – це беггінг над деревами рішень, під час навчання яких, для кожного розбиття, ознаки вибираються з деякої випадкової підмножини ознак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.2. Порівняння випадкого лісу з деревом рішень і беггінгом</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Підключимо пакети `sklearn` для навчання та тестування моделей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведемо порівняння моделей на основі дерева рішень, беггінга з дерев рішень та випадкового лісу для **задачі регресії**.\n",
    "\n",
    "Сформуємо навчальний та тестові набори даних:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 150        \n",
    "n_test = 1000       \n",
    "noise = 0.1\n",
    "\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, input_noise):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X).ravel()\n",
    "    y = np.exp(-X ** 2) + 1.5 * np.exp(-(X - 2) ** 2)\\\n",
    "        + np.random.normal(0.0, input_noise, n_samples)\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate(n_samples=n_train, input_noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, input_noise=noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо навчання регресійної моделі на основі одного дерева рішень:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeRegressor()\n",
    "dtree.fit(X_train, y_train)\n",
    "d_predict = dtree.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, d_predict, \"g\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Дерево рішень, MSE = %.2f\" \n",
    "          % np.sum((y_test - d_predict) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо навчання регресійної моделі на основі беггінгу з дерев рішень:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt = BaggingRegressor(DecisionTreeRegressor()).fit(X_train, y_train)\n",
    "bdt_predict = bdt.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, bdt_predict, \"y\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Беггінг з дерев рішень, MSE = %.2f\"\n",
    "          % np.sum((y_test - bdt_predict) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо навчання регресійної моделі на основі випадкового лісу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10).fit(X_train, y_train)\n",
    "rf_predict = rf.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, rf_predict, \"r\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Випадковий ліс, MSE = %.2f\"\n",
    "          % np.sum((y_test - rf_predict) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як ми бачимо з графіків і значень помилки MSE, випадковий ліс з 10 дерев дає кращий результат, ніж одне дерево або беггінг з 10 дерев рішень. Основна відмінність випадкового лісу й беггінга проти дерев рішень полягає в тому, що у випадковому лісі з випадкової кількості дерев вибирається випадкова підмножина ознак, і найкраща ознака для поділу вузла визначається з підвибірки ознак, на відміну від Безгіна, де всі функції розглядаються для поділу у вузлі дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі проведемо перевірку розглядуваних підходів для **задачі класифікації**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X, y = make_circles(n_samples=500, factor=0.1, noise=0.35, random_state=42)\n",
    "X_train_circles, X_test_circles, y_train_circles, y_test_circles = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо навчання моделі класифікації на основі дерева рішень:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train_circles, y_train_circles)\n",
    "\n",
    "x_range = np.linspace(X.min(), X.max(), 100)\n",
    "xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "y_hat = dtree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "y_hat = y_hat.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, y_hat, alpha=0.2)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='autumn')\n",
    "plt.title(\"Дерево рішень\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо навчання моделі класифікації на основі беггінгу з дерева рішень:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dtree = BaggingClassifier(DecisionTreeClassifier(),n_estimators=300, random_state=42)\n",
    "b_dtree.fit(X_train_circles, y_train_circles)\n",
    "\n",
    "x_range = np.linspace(X.min(), X.max(), 100)\n",
    "xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "y_hat = b_dtree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "y_hat = y_hat.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, y_hat, alpha=0.2)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='autumn')\n",
    "plt.title(\"Беггінг з дерев рішень\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Виконаємо навчання моделі класифікації на основі випадкового лісу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train_circles, y_train_circles)\n",
    "\n",
    "x_range = np.linspace(X.min(), X.max(), 100)\n",
    "xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "y_hat = rf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "y_hat = y_hat.reshape(xx1.shape)\n",
    "plt.contourf(xx1, xx2, y_hat, alpha=0.2)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='autumn')\n",
    "plt.title(\"Випадковий ліс\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відповідно до графіків вище, границя, що розділяє дерева рішень дуже «рвана» і на ній багато гострих кутів, що говорить про перенавчання й слабку узагальнюючу здатність. Водночас як беггінг, так і випадковий ліс демонструють досить згладжену границю, що свідчить про відсутність перенавчання.\n",
    "\n",
    "Далі спробуємо розібратися з параметрами, підбираючи значення яких, ми зможемо збільшити значення частки правильних відповідей (ЧПВ, accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.3. Параметри випадкового лісу</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод випадкового лісу реалізований в бібліотеці МН [scikit-learn](http://scikit-learn.org/stable/) двома класами RandomForestClassifier і RandomForestRegressor.\n",
    "\n",
    "Повний список параметрів випадкового лісу для задачі регресії:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class sklearn.ensemble.RandomForestRegressor(`\n",
    "   + n_estimators – кількість дерев в \"лісі\" (за замовчуванням – 10);\n",
    "   + criterion – функція, яка вимірює якість розбиття гілки дерева (за замовчуванням – 1\"mse\", можна вибрати \"mae\");\n",
    "   + max_features – кількість ознак, за якими шукається розбиття; можна вказати конкретну кількість або відсоток ознак, або вибрати з доступних значень: \"auto\" (всі ознаки), \"sqrt\", \"log2\"; за замовчуванням – \"auto\";\n",
    "   + max_depth – максимальна глибина дерева (за замовчуванням не обмежена);\n",
    "   + min_samples_leaf – мінімальна кількість об'єктів в листі; можна задати кількістю або відсотком від загальної кількості об'єктів (за замовчуванням – 1);\n",
    "   + min_samples_split – мінімальна кількість об'єктів, що необхідна для поділу внутрішнього вузла; можна задати кількістю або відсотком від загальної кількості об'єктів (за замовчуванням – 2);\n",
    "   + min_weight_fraction_leaf – мінімальна зважена частка від загальної суми ваг (всіх вхідних об'єктів) має бути в листі (за замовчуванням мають однакову вагу);\n",
    "   + max_leaf_nodes – максимальна кількість листів (за замовчуванням немає обмеження);\n",
    "   + min_impurity_split – поріг для зупинки нарощування дерева (за замовчуванням – 1е-7);\n",
    "   + bootstrap – застосовувати бустреп для побудови дерева (за замовчуванням – True);\n",
    "   + oob_score – чи використовувати out-of-bag об'єкти для оцінки R^2 (за замовчуванням – False);\n",
    "   + n_jobs – кількість ядер для побудови моделі та передбачень (за замовчуванням – 1, якщо підставити -1, то будуть використовуватися всі ядра);\n",
    "   + random_state – початкове значення для генерації випадкових чисел (за замовчуванням його немає; якщо хочете відтворювані результати, то потрібно вказати будь-яке число за типом int);\n",
    "   + verbose – висновок логів за побудови дерев (за замовчуванням – 0);\n",
    "   + warm_start – використовує вже навчену модель і додає дерева в ансамбль (за замовчуванням – False)\n",
    "\n",
    "`)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачі класифікації все майже те ж саме, наведемо лише ті параметри, за якими RandomForestClassifier відрізняється від RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`class sklearn.ensemble.RandomForestClassifier(`\n",
    "   + criterion – оскільки тут маємо модель класифікації, то за замовчуванням обрано критерій \"gini\" (можна вибрати \"entropy\");\n",
    "   + class_weight – вага кожного класу (за замовчуванням всі ваги дорівнюють 1, але можна передати словник з вагами, або явно вказати \"balanced\", тоді ваги класів будуть рівні їхнім початковим частинам у генеральній сукупності; також можна вказати \"balanced_subsample\", тоді ваги за кожною підвибіркою будуть змінюватися в залежності від розподілу класів за цією підвибіркою\n",
    "    \n",
    "`)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі розглянемо кілька параметрів, на які в першу чергу варто звернути увагу під час побудови моделі:\n",
    "\n",
    "- `n_estimators` – кількість дерев в \"лісі\";\n",
    "- `criterion` – критерій для розбиття вибірки у вершині;\n",
    "- `max_features` – кількість ознак, за якими відбувається пошук розбиття;\n",
    "- `min_samples_leaf` – мінімальна кількість об'єктів в листі;\n",
    "- `max_depth` – максимальна глибина дерева."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо застосування випадкового лісу до реальної задачі.\n",
    "\n",
    "Використаємо приклад із задачею виявлення шахрайства (з англ. fraud detection). Це задача класифікації, тому будемо використовувати метрику accuracy для оцінювання ЧПВ. Для початку побудуємо найпростіший класифікатор, що матиме роль базової моделі (з англ. baseline). Візьмемо тільки числові ознаки для спрощення."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Завантажуємо дані\n",
    "df = pd.read_csv(telecom_churn_url)\n",
    "\n",
    "# Вибираємо спочатку тільки колонки з числовим типом даних\n",
    "cols = []\n",
    "for i in df.columns:\n",
    "    if (df[i].dtype == \"float64\") or (df[i].dtype == 'int64'):\n",
    "        cols.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Поділяємо на об'єкти й ознаки\n",
    "X, y = df[cols].copy(), np.asarray(df[\"Churn\"],dtype='int8')\n",
    "\n",
    "# Ініціалізуємо стратифіковане розбиття нашого набору даних для тестування\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Ініціалізуємо наш класифікатор з параметрами за замовчуванням\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "# Виконуємо навчання моделі за навчальним набором даних\n",
    "results = cross_val_score(rfc, X, y, cv=skf)\n",
    "\n",
    "# Оцінюємо ЧПВ за тестовим набором даних\n",
    "print(f\"Cross validation accuracy score: {format(results.mean()*100):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отримали ЧПВ 92.50%. Тепер спробуємо покращити цей результат і подивимося, як поводяться криві навчання у випадку зміни основних параметрів.\n",
    "\n",
    "Почнемо з кількості дерев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ініціалізуємо тестування\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Створюємо списки для збереження ЧПВ за навчальним та тестовим наборами даних\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "trees_grid = [5, 10, 15, 20, 30, 50, 75, 100]\n",
    "\n",
    "# Проводимо навчання за навчальним набором даних\n",
    "for ntrees in trees_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=ntrees, random_state=42, n_jobs=-1, oob_score=True)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "test_max_cv = max(test_acc.mean(axis=1))*100\n",
    "test_num_trees = trees_grid[np.argmax(test_acc.mean(axis=1))]\n",
    "print(f\"Best accuracy on CV is {test_max_cv:.2f}% with {test_num_trees} trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(trees_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(trees_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='test')\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(trees_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"N_estimators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "З графіку вище бачимо, що за досягнення певної кількості дерев наша accuracy за тестовим набором даних виходить на асимптоту, і тому ви можете самостійно обрати, яка кількість дерев є оптимальною для вашої задачі.\n",
    "\n",
    "Далі спробуємо додати параметри регуляризації в модель.\n",
    "\n",
    "Почнемо з параметра максимальної глибини – `max_depth`. (Зафіксуємо кількість дерев 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створюємо списки для збереження оцінки ЧПВ за навчальним та тестовим набороми даних\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "max_depth_grid = [3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n",
    "\n",
    "# Проводимо навчання за навчальним набором даних\n",
    "for max_depth in max_depth_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, oob_score=True, max_depth=max_depth)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "test_max_cv = max(test_acc.mean(axis=1))*100\n",
    "test_max_depth_grid = max_depth_grid[np.argmax(test_acc.mean(axis=1))]\n",
    "print(f\"Best accuracy on CV is {test_max_cv:.2f}% with {test_max_depth_grid} max_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(max_depth_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(max_depth_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='test')\n",
    "ax.fill_between(max_depth_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(max_depth_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Max_depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр `max_depth` добре справляється з регуляризацією моделі, і модель вже не так сильно перенавчається. ЧПВ нашої моделі трохи зросла.\n",
    "\n",
    "Ще важливим є параметр `min_samples_leaf`, він також виконує функцію регуляризатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створюємо списки для збереження оцінки ЧПВ за навчальним та тестовим набороми даних\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "min_samples_leaf_grid = [1, 3, 5, 7, 9, 11, 13, 15, 17, 20, 22, 24]\n",
    "\n",
    "# Виконуємо навчання за навчальним набором даних\n",
    "for min_samples_leaf in min_samples_leaf_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n",
    "                                 oob_score=True, min_samples_leaf=min_samples_leaf)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "test_max_cv = max(test_acc.mean(axis=1))*100\n",
    "test_min_samples_leaf_grid = min_samples_leaf_grid[np.argmax(test_acc.mean(axis=1))]\n",
    "print(f\"Best accuracy on CV is {test_max_cv:.2f}% with {test_min_samples_leaf_grid} min_samples_leaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(min_samples_leaf_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(min_samples_leaf_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='test')\n",
    "ax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(min_samples_leaf_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Min_samples_leaf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут ми не виграємо в ЧПВ за тестуванням, але зате можемо сильно зменшити перенавчання до 2% при збереженні ЧПВ близько 92%.\n",
    "\n",
    "Розглянемо такий параметр як `max_features`. Для задач класифікації за замовчуванням використовується $\\sqrt{n}$, де $n$ – кількість ознак. Далі перевіримо, чи оптимально в нашому випадку використовувати 4 ознаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створюємо списки для збереження оцінки ЧПВ за навчальним та тестовим набороми даних\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "temp_train_acc = []\n",
    "temp_test_acc = []\n",
    "max_features_grid = [2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "# Виконуємо навчання за навчальним набором даних\n",
    "for max_features in max_features_grid:\n",
    "    rfc = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, \n",
    "                                 oob_score=True, max_features=max_features)\n",
    "    temp_train_acc = []\n",
    "    temp_test_acc = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        rfc.fit(X_train, y_train)\n",
    "        temp_train_acc.append(rfc.score(X_train, y_train))\n",
    "        temp_test_acc.append(rfc.score(X_test, y_test))\n",
    "    train_acc.append(temp_train_acc)\n",
    "    test_acc.append(temp_test_acc)\n",
    "    \n",
    "train_acc, test_acc = np.asarray(train_acc), np.asarray(test_acc)\n",
    "test_max_cv = max(test_acc.mean(axis=1))*100\n",
    "test_max_features_grid = max_features_grid[np.argmax(test_acc.mean(axis=1))]\n",
    "print(f\"Best accuracy on CV is {test_max_cv:.2f}% with {test_max_features_grid} max_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(max_features_grid, train_acc.mean(axis=1), alpha=0.5, color='blue', label='train')\n",
    "ax.plot(max_features_grid, test_acc.mean(axis=1), alpha=0.5, color='red', label='test')\n",
    "ax.fill_between(max_features_grid, test_acc.mean(axis=1) - test_acc.std(axis=1), test_acc.mean(axis=1) + test_acc.std(axis=1), color='#888888', alpha=0.4)\n",
    "ax.fill_between(max_features_grid, test_acc.mean(axis=1) - 2*test_acc.std(axis=1), test_acc.mean(axis=1) + 2*test_acc.std(axis=1), color='#888888', alpha=0.2)\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim([0.88,1.02])\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"Max_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нашому випадку оптимальна кількість ознак – 4, саме з таким значенням досягається найкращий результат.\n",
    "\n",
    "Ми розглянули, як поводяться криві навчання в залежності від зміни основних параметрів. Давайте тепер за допомогою `GridSearchCV` знайдемо оптимальні параметри для нашого прикладу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проведемо ініціалізацію параметрів, за якими хочемо зробити повний перебір\n",
    "parameters = {'max_features': [4, 7, 10, 13], 'min_samples_leaf': [1, 3, 5, 7], 'max_depth': [5, 10, 15, 20]}\n",
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42, \n",
    "                             n_jobs=-1, oob_score=True)\n",
    "gcv = GridSearchCV(rfc, parameters, n_jobs=-1, cv=skf, verbose=1)\n",
    "gcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcv.best_estimator_, gcv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краща ЧПВ, який ми змогли досягти за допомогою перебору параметрів – 92.56% за `'max_depth': 20, 'max_features': 10, 'min_samples_leaf': 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.4. Варіація й декореляційний ефект</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подамо варіацію для випадкового лісу як\n",
    "\n",
    "$$\\large Varf(x) = \\rho(x)\\sigma^2(x) $$\n",
    "\n",
    "де $\\rho(x)$ – кореляція вибірки між будь-якими двома деревами, що використовуються за усереднення:\n",
    "\n",
    "$$ \\large \\rho(x) = corr[T(x;\\Theta_1(Z)),T(x_2,\\Theta_2(Z))],$$\n",
    "\n",
    "де $\\Theta_1(Z) $ та $\\Theta_2(Z) $ – випадково обрана пара дерев на випадково обраних об'єктах вибірки $Z$;\n",
    "\n",
    "$\\sigma^2(x)$ – це вибіркова дисперсія будь-якого довільно обраного дерева:\n",
    "\n",
    "$$ \\large \\sigma^2(x) = VarT(x;\\Theta(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко сплутати $\\rho(x)$ з середньою кореляцією між навченими деревами в даному випадковому лісі, розглядаючи дерева як $N$-вектори і обчислюючи середню парну кореляцію між ними. Це не той випадок. Ця умовна кореляція не має прямого відношення до процесу усереднення, а залежність від $x$ в $\\rho(x)$ попереджає нас про цю відмінність. Краще сказати, що $\\rho(x)$ є теоретичною кореляцією між парою випадкових дерев, що оцінені в об'єкті $x$, яка була викликана багаторазовим розбиттям навчальної вибірки з генеральної сукупності $Z$, і після цього обрана дана пара випадкових дерев. На статистичному жаргоні це кореляція, що викликана вибірковим розподілом $Z$ і $\\Theta$.\n",
    "\n",
    "За фактом, умовна коваріація пари дерев дорівнює 0, тому що бустреп і відбір ознак – незалежні та однаково розподілені."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якщо розглянути дисперсію за одним деревом, то вона практично не змінюється від змінних для поділу ($m$), а ось для ансамблю це грає велику роль, і дисперсія для дерева набагато вища, ніж для ансамблю. У книзі *The Elements of Statistical Learning (Trevor Hastie, Robert Tibshirani і Jerome Friedman)*, [PDF](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) є відмінний приклад, який це демонструє."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_5_variance_rf.png\" align=\"center\" width=100% height=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.5. Зміщення</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Як і для беггінгу, зміщення у випадковому лісі таке ж, як і зміщення в окремо взятому дереві $T(x,\\Theta(Z))$:\n",
    "\n",
    "$$ \\large \\begin{array}{rcl} Bias &=& \\mu(x) – E_Zf_{rf}(x) \\\\\n",
    "&=& \\mu(x) – E_ZE_{\\Theta | Z}T(x,\\Theta(Z))\\end{array} $$\n",
    "\n",
    "Це також зазвичай більше (в абсолютних величинах), ніж зміщення «неусеченного» (unprunned) дерева, оскільки рандомізація та скорочення простору вибірки накладають обмеження. Отже, покращення в прогнозуванні, що отримані за допомогою беггінга або випадкових лісів, є виключно результатом зменшення дисперсії."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.6. Понад випадкові дерева</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У понад випадкових деревах ([Extremely Randomized Trees](https://link.springer.com/article/10.1007/s10994-006-6226-1)) більше випадковості в тому, як обчислюються поділи у вузлах. Як і в випадкових лісах, використовується випадкова підмножина можливих ознак, але замість пошуку оптимальних порогів, порогові значення довільно вибираються для кожної можливої ознаки, і найкращий з цих випадково сформованих порогів вибирається як краще правило для поділу вузла. Такий підхід зазвичай дає змогу трохи зменшити дисперсію моделі через трохи більший зсув.\n",
    "\n",
    "У бібліотеці scikit-learn є реалізація [ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) и [ExtraTreesRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor). Даний метод варто використовувати, коли ваша модель сильно перенавчається на випадковому лісі або градієнтному бустингу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.7. Схожість випадкового лісу з алгоритмом *k*-найближчих сусідів</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод випадкового лісу схожий з методом найближчих сусідів. Випадкові ліси здійснюють передбачення для об'єктів на основі міток схожих об'єктів з навчання. Схожість об'єктів водночас тим вища, чим частіше ці об'єкти перебувають в одному і тому ж листі дерева. Покажемо це формально.\n",
    "\n",
    "Розглянемо задачу регресії з квадратичною функцією втрат. Нехай $T_n(x)$ – номер листа $n$-го дерева з випадкового лісу, в який потрапляє об'єкт $x$. Відповідь об'єкта $x$ дорівнює середньому поміж відповідей за всіма об'єктами навчальної вибірки, які потрапили в цей лист $T_n(x)$. Це можна записати так\n",
    "\n",
    "$$\\large b_n(x) = \\sum_{i=1}^{l}w_n(x,x_i)y_i,$$ \n",
    "\n",
    "де $$ \\large w_n(x, x_i) = \\frac{[T_n(x) = T_n(x_i)]}{\\sum_{j=1}^{l}[T_n(x) = T_n(x_j)]}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоді відповідь композиції дорівнює\n",
    "\n",
    "$$ \\large \\begin{array}{rcl} a_n(x) &=& \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{i=1}^{l}w_n(x,x_i)y_i \\\\\n",
    "&=& \\sum_{i=1}^{l}\\Big(\\frac{1}{N}\\sum_{j=1}^{N}w_n(x,x_j)\\Big)y_i \\end{array}.$$\n",
    "\n",
    "Бачимо, що результат випадкового лісу є сумою результатів всіх об'єктів навчання з деякими вагами. Зазначимо, що номер листа $T_n(x)$, в який потрапив об'єкт, є цінною ознакою. Досить непогано працює підхід, в якому за вибіркою навчається композиція з невеликої кількості дерев за допомогою випадкового лісу або градієнтного бустингу, а потім до неї додаються категоріальні ознаки $T_1(x), \\dots, T_n(x)$. Нові ознаки є результатом нелінійного розбиття простору й несуть в собі інформацію щодо подібності об'єктів.\n",
    "\n",
    "У книзі *The Elements of Statistical Learning* ([PDF](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)) є хороший наочний приклад подібності випадкового лісу й *k*-найближчих сусідів."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_6_knn_vs_rf.png\" align=\"center\" width=1000% height=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.2.8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.2.8. Перетворення ознак в багатовимірний простір</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всі звикли використовувати випадковий ліс для задач навчання з учителем, але також є можливість проводити навчання і без учителя. За допомогою методу [RandomTreesEmbedding](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding) ми можемо зробити трансформацію нашого набору даних в багатовимірне розріджене його уявлення. Суть цього методу в тому, що будуються абсолютно випадкові дерева, й індекс листа, в якому опинилося спостереження, вважається новою ознакою. Якщо в перший лист потрапив об'єкт, то йому призначається індекс 1, а якщо не потрапив, то – 0. Маємо так зване бінарне кодування.\n",
    "\n",
    "Можемо контролювати кількість змінних і також ступінь розрідження нового уявлення набору даних збільшуючи або зменшуючи кількість дерев і їхні глибини. Оскільки сусідні точки даних, швидше за все, лежать в одному і тому ж листі дерева, перетворення виконує неявну, непараметричну оцінку щільності."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.3. Важливість ознак на прикладі випадкового лісу</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Під час розв'язання реальних задач часто виникає необхідність зрозуміти роботу алгоритму, який ми використовуємо, чому він дає певну відповідь саме так, а не інакше тощо. Або якщо не вдається зрозуміти алгоритм повністю, то зазвичай потрібно розуміти які змінні алгоритму мають найбільш суттєвий вплив на кінцевий результат. З випадкового лісу можна досить просто отримати таку інформацію."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.3.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.3.1. Метод випадкового лісу</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За рисунком нижче інтуїтивно зрозуміло, що важливість ознаки «Вік» у задачі кредитного скорингу вище, ніж важливість ознаки «Дохід». Формалізується це за допомогою поняття приросту інформації."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/03_img/5_7_credit_scoring_toy_tree_english.png\" align=\"center\" width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Якщо побудувати багато дерев рішень (випадковий ліс), то чим вищою в середньому є ознака в дереві рішень, тим важливішою вона є в даній задачі класифікації/регресії. У разі кожного розбиття в кожному дереві покращення критерію поділу (в нашому випадку коефіцієнт Джині) – це показник важливості, що пов'язаний зі змінною поділу, і накопичується він за усіма деревами лісу окремо для кожної змінної."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте трохи заглибимося в деталі. Середнє зниження точності, що викликається змінною, визначається під час фази обчислення out-of-bag помилки. Чим більше зменшується точність передбачень через виключення (або перестановку) однієї змінної, тим важливішою є ця змінна, і тому змінні з більшим середнім зменшенням точності важливіші для класифікації даних. Середнє зменшення коефіцієнта Джині (або помилки MSE в задачах регресії) є мірою того, як кожна змінна сприяє однорідності вузлів і листя в кінцевій моделі випадкового лісу. Кожен раз, коли окрема змінна використовується для розбиття вузла, коефіцієнт Джині для дочірніх вузлів розраховується і порівнюється з коефіцієнтом початкового вузла. Коефіцієнт Джині є мірою однорідності від 0 (однорідної) до 1 (гетерогенної). Зміни в значенні критерію поділу підсумовуються для кожної змінної і нормуються в кінці обчислення. Змінні, які призводять до вузлів з більш високою чистотою, мають більш високе зниження коефіцієнта Джині."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далі подамо вказані вище слова у вигляді формул:\n",
    "\n",
    "$$ \\large VI^{T} = \\frac{\\sum_{i \\in \\mathfrak{B}^T}I \\Big(y_i=\\hat{y}_i^{T}\\Big)}{\\Big |\\mathfrak{B}^T\\Big |} – \\frac{\\sum_{i \\in \\mathfrak{B}^T}I \\Big(y_i=\\hat{y}_{i,\\pi_j}^{T}\\Big)}{\\Big |\\mathfrak{B}^T\\Big |}, $$\n",
    "\n",
    "де\n",
    "$ \\hat{y}_i^{(T)} = f^{T}(x_i)  $ – передбачення класу перед перестановкою/видаленням ознаки;\n",
    "\n",
    "$ \\hat{y}_{i,\\pi_j}^{(T)} = f^{T}(x_{i,\\pi_j}) $ – передбачення класу після перестановки/видалення ознаки;\n",
    "\n",
    "$  x_{i,\\pi_j} = (x_{i,1}, \\dots , x_{i,j-1}, \\quad x_{\\pi_j(i),j}, \\quad x_{i,j+1}, \\dots , x_{i,p}).$\n",
    "\n",
    "Помітно, що $ VI^{(T)}(x_j) = 0 $, якщо $ X_j $  не перебуває у дереві $ T $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розрахунок важливості ознак в ансамблі:\n",
    "\n",
    "- ненормовані:\n",
    "\n",
    "$$ \\large VI(x_j) = \\frac{\\sum_{T=1}^{N}VI^{T}(x_j)}{N}; $$\n",
    "\n",
    "- нормовані:\n",
    "$$ \\large z_j = \\frac{VI(x_j)}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.3.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue; font-size:1em;\">5.3.2. Приклад розрахування важливості ознак</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розглянемо результати анкетування відвідувачів хостелів з сайтів Booking.com і TripAdvisor.com. Тут ознаки – середні оцінки за різними чинниками (перераховані нижче) – персонал, стан кімнат тощо. Цільова ознака – рейтинг хостела на сайті."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оголосимо україномовні заголовки\n",
    "from matplotlib import rc\n",
    "font = {'family': 'Verdana',\n",
    "        'weight': 'normal'}\n",
    "rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hostel_factors_url = 'https://raw.githubusercontent.com/radiukpavlo/intelligent-data-analysis/main/01_lecture-notes/ida_lecture-05_compositions/hostel_factors.csv'\n",
    "\n",
    "hostel_data = pd.read_csv(hostel_factors_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hostel_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\"f1\":u\"Персонал\",\n",
    "\"f2\":u\"Бронювання хостела\",\n",
    "\"f3\":u\"Заїзд в хостел і виїзд з нього\",\n",
    "\"f4\":u\"Стан кімнати\",\n",
    "\"f5\":u\"Стан спільної кухні\",\n",
    "\"f6\":u\"Стан спільного простору\",\n",
    "\"f7\":u\"Додаткові послуги\",\n",
    "\"f8\":u\"Загальні умови та зручності\",\n",
    "\"f9\":u\"Ціна/якість\",\n",
    "\"f10\":u\"Спільне створення цінності\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створюємо модель випадкового лісу для задачі регресії\n",
    "forest = RandomForestRegressor(n_estimators=1000, max_features=10,\n",
    "                                random_state=0)\n",
    "\n",
    "forest.fit(hostel_data.drop(['hostel', 'rating'], axis=1),\n",
    "           hostel_data['rating'])\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Зображаємо важливість ознак випадкового лісу\n",
    "num_to_plot = 10\n",
    "feature_indices = [ind + 1 for ind in indices[:num_to_plot]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Виводимо рейтинг ознак\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(num_to_plot):\n",
    "    print(\"%d. %s %f \" % (f + 1, \n",
    "            features[\"f\"+str(feature_indices[f])], \n",
    "            importances[indices[f]]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(u\"Важливість конструктів\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bars = plt.bar(range(num_to_plot),\n",
    "               importances[indices[:num_to_plot]],\n",
    "               color=([str(i/float(num_to_plot+1))\n",
    "                       for i in range(num_to_plot)]),\n",
    "               align=\"center\")\n",
    "\n",
    "ticks = plt.xticks(range(num_to_plot), feature_indices)\n",
    "plt.xlim([-1, num_to_plot])\n",
    "plt.legend(bars, [u''.join(features[\"f\"+str(i)])\n",
    "                  for i in feature_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На графіку вище бачимо, що люди найбільше звертають увагу на персонал і співвідношення ціна/якість, та на основі враження від цих ознак пишуть свої відгуки. Водночас різниця між цими значущими та менш впливовими ознаками не велика, і тому відкидання якоїсь ознаки призведе до зменшення точності нашої моделі. Втім навіть на основі нашого аналізу ми можемо дати рекомендації готелям в першу чергу краще готувати персонал і/або покращити якість номерів до заявленої ціни."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.4. Переваги та недоліки використання випадкових лісів в ІАД та МН</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Переваги**:\n",
    " \n",
    " - має високу точність передбачення (accuracy, частка правильних відповідей); на більшості завдань буде краще лінійних алгоритмів; значення ЧПВ відповідає значенню бустинга;\n",
    " - практично не чутливий до викидів у даних через випадкове розбиття;\n",
    " - нечутливий до масштабування (і взагалі до будь-яких монотонних перетворень) значень ознак; це пов’язане з вибором випадкових підпросторів;\n",
    " - не потребує ретельного налаштування гіперпараметрів, добре працює «з коробки»; за допомогою «тюнінгу» гіперпараметрів можна досягти приросту від 0.5 до 3 % ЧПВ залежно від задачі й даних;\n",
    " - може ефективно обробляти дані з великою кількістю ознак і класів;\n",
    " - однаково добре обробляє як неперервні, так і дискретні ознаки;\n",
    " - рідко перенавчається, на практиці додавання дерев майже завжди тільки покращує композицію; проте за валідацією, після досягнення певної кількості дерев, крива навчання виходить на асимптоту;\n",
    " - для випадкового лісу є методи оцінювання значущості окремих ознак у моделі;\n",
    " - добре працює з пропущеними даними; зберігає хорошу точність, якщо більша частина даних пропущені;\n",
    " - передбачає можливість збалансувати вагу кожного класу за всією вибіркою, або за підвибірками кожного дерева;\n",
    " - обчислює близькість між парами об’єктів, які можуть використовуватися в разі кластеризації, виявлення викидів або (через масштабування) дають цікаві уявлення даних;\n",
    " - можливості, що описані вище, можуть бути розширені до немаркованих даних, що дає змогу виконувати кластеризацію та візуалізацію даних, виявляти викиди;\n",
    " - високий ступінь паралелезованості й масштабованості."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Недоліки**:\n",
    " \n",
    " - результати випадкового лісу складніше інтерпретувати проти дерев рішень;\n",
    " - немає формальних висновків (p-values), що доступні для оцінювання важливості змінних;\n",
    " - алгоритм працює гірше багатьох лінійних методів, коли у вибірці дуже багато розріджених ознак (наприклад, тексти або bag of words);\n",
    " - випадковий ліс не вміє екстраполювати проти тієї ж лінійної регресії (але це можна вважати й перевагою, тому що не буде екстремальних значень у разі потрапляння викиду);\n",
    " - алгоритм схильний до перенавчання в деяких задачах, особливо за зашумленими даними;\n",
    " - для даних, що включають категоріальні змінні з різною кількістю рівнів, випадковий ліс упереджений на користь ознак із великою кількістю рівнів: коли в ознаки багато рівнів, дерево буде сильніше підлаштовуватися саме під ці ознаки, позаяк на них можна отримати вищу оцінку ЧПВ;\n",
    " - якщо дані містять групи корельованих ознак, що мають схожу значущість для міток, то перевага віддається невеликій групі перед великими;\n",
    " - більший розмір результуючих моделей; потрібно $O(NK)$ пам’яті для зберігання моделі, де $K$ – кількість дерев випадкового лісу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue; font-size:1.2em;\">5.5. Корисні посилання</span>\n",
    "\n",
    "[Повернутися до змісту](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 15-ий розділ книги “[Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)” Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie. [ENG]\n",
    "- Велика обзорна [стаття](https://neptune.ai/blog/ensemble-learning-guide) про ансамблеві алгоритми. [ENG]\n",
    "- Більше про практичні застосування випадкового лісу та інших алгоритмів композицій в [офіційній документації](http://scikit-learn.org/stable/modules/ensemble.html) scikit-learn [ENG].\n",
    "- [Матриця невідповідностей](https://en.wikipedia.org/wiki/Confusion_matrix) для характеристики класифікатор [ENG]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
